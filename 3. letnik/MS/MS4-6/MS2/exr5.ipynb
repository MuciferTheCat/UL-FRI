{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Text Retrieval\n",
    "\n",
    "\n",
    "To complete the exercise, follow the instructions and complete the missing code and write the answers where required.  All points, except the ones marked with **(N points)** are mandatory. The optional tasks require more independet work and some extra effort. Without completing them you can get at most 75 points for the exercise (the total number of points is 100 and results in grade 10). Sometimes there are more optional exercises and you do not have to complete all of them, you can get at most 100 points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this exercise you will implement some indexing operations used to retrieve information from a corpus of text documents. As the size of readily available text documents grows beyond all measures, methods for fast and user-friendly querying of information from text files are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import math\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "import sys   \n",
    "PYTHONIOENCODING=\"UTF-8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Using nltk library and corpuses\n",
    "\n",
    "In this assignment, you will learn how to use <a href=\"https://www.nltk.org/\"><b>nltk</b> library</a> and how to load and preprocess a corpus of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) The <b>nltk</b> library includes mechanisms for loading a number of different text corpuses. Load the <b>Gutenberg corpus</b> by using the function ``nltk.download()``. Then, with the help of the <a href=\"http://www.nltk.org/book/\">NLTK Book</a>, familiarize yourself with the contents and structure of the corpus. If at any point of the exercise you find the Gutenberg corpus too small or otherwise unsuitable for your needs you are welcome to try other corpuses available on the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download Gutenberg corpus\n",
    "nltk.download('gutenberg')\n",
    "# Download Punkt Tokenizer Models\n",
    "nltk.download('punkt')\n",
    "# Download Stop Words\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Show structure of Gutenberg corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: familiarize/experiment with the content of Gutenberg corpus\n",
    "#       e.g. show the number of words, display first few words, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Preprocess the raw data of each document in the Gutenberg corpus using the nltk inbuilt tokenizer. You can use the function ``nltk.word_tokenize()``. Remove stop words and punctuation to further reduce the amount of data you will need to process later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Write a function that recives an array of words obtained from tokenized file\n",
    "# as an input parameter and returns an array of filtered words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Preprocess raw data of documents in Gutenberg corpus (use build in tokenizer 'word_tokenize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) <b>(15 points)</b> Write your own tokenizer for preprocessing. You can use regular expressions. Show the difference in the results from the tokenizer you used in the previous task. What kind of tokens did you keep/ignore relative to the in-built tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Bonus +15 points)\n",
    "# 1) Write your own tokenizer\n",
    "# 2) Compore results with the inbuilt tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Indexing\n",
    "\n",
    "In order to simplify and speed up operations on sets of documents, they need to be indexed. This allows a fast look up if and where a query word or phrase appears in our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Build an inverted index. For each unique token appearing in your corpus, make a list of the documents it appears in. Make some queries using Boolean logic (operation AND is an intersection of lists etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Build an inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Make some queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) To allow querying of tokens that occur together (i.e. common phrases), a positional index can be used. Build a positional index on your corpus. This is an extension of the inverted index where each of the list elements containing the document index also stores a list of positions in the document where the token appears. Make sure you properly removed stop words in order to keep your computation relatively fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Build positional index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Make some queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Use the positional index to query phrases. That is, return the positions in documents where each of the words in your phrase occurs at approximately the same position in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Return the positions in documents where each of the words in your query phrase\n",
    "# occurs at approximately the same position in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Text statistics\n",
    "\n",
    "a) The relevance of the documents in your corpus to the user's query can be measured by the term frequency, i.e. how many times the query term appears in each document. Implement a function that counts the number of appearances of each token in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Count the number of appearances of each token in each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) The absolute number of appearances is biased, therefore a different metric called <b>TFIDF</b> (short for term frequencyâ€“inverse document frequency) is commonly used to rank the relevance of documents containing a query. TFIDF is computed as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{tfidf}_{t,d} = \\mathrm{tf}_{t,d} \\cdot \\mathrm{idf}_t,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathrm{tf}_{t,d}$ is the frequency of the term $t$ in document $d$ and $\\mathrm{idf}_{t}$ equals to\n",
    "\n",
    "\\begin{equation}\n",
    "\\log_{10}{\\frac{N}{\\mathrm{df}_t}},\n",
    "\\end{equation}\n",
    "\n",
    "where $N$ is the number of documents in the corpus and $\\mathrm{df}_t$ is the number of documents of the corpus in which the term $t$ appears.\n",
    "\n",
    "Implement a system that returns the first $5$ most relevant documents from the corpus given a query. Note that your queries can contain more than one word. The score in that case is calculated as\n",
    "\n",
    "\\begin{equation}\n",
    "s(q,d) = \\sum_{t \\in q}{\\mathrm{tfidf}_{t,d}},\n",
    "\\end{equation}\n",
    "\n",
    "where $q$ is your query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Return 5 most relevant documents from the corpus given a query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) <b>(10 points)</b> Implement a system for handling typographical errors of queries on the user's part. The choice of the method is up to you. You need to show that your system returns relevant results for misspelled queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Bonus +10 points)\n",
    "# Handle typographical errors in user's query and return relevant results for misspelled queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
